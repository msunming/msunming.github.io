<!DOCTYPE html>
	<html>
		<meta http-equiv="content-type" content="text/html;charset=UTF-8">

   <style type="text/css">

   /*
CSS stylesheet is based on killwing's flavored markdown style:
https://gist.github.com/2937864
*/
body{
    margin: 0 auto;
    font: 13px/1.231 Helvetica, Arial, sans-serif;
    color: #444444;
    line-height: 1;
    max-width: 960px;
    padding: 5px;
}
h1, h2, h3, h4 {
    color: #111111;
    font-weight: 400;
}
h1, h2, h3, h4, h5, p {
    margin-bottom: 16px;
    padding: 0;
}
h1 {
    font-size: 28px;
}
h2 {
    font-size: 22px;
    margin: 20px 0 6px;
}
h3 {
    font-size: 21px;
}
h4 {
    font-size: 18px;
}
h5 {
    font-size: 16px;
}
a {
    color: #0099ff;
    margin: 0;
    padding: 0;
    vertical-align: baseline;
}
a:link,a:visited{
 text-decoration:none;
}
a:hover{
 text-decoration:underline;
}
ul, ol {
    padding: 0;
    margin: 0;
}
li {
    line-height: 24px;
    margin-left: 44px;
}
li ul, li ul {
    margin-left: 24px;
}
ul, ol {
    font-size: 14px;
    line-height: 20px;
    max-width: 540px;
}

p {
    font-size: 14px;
    line-height: 20px;
    max-width: 540px;
    margin-top: 3px;
}

pre {
    padding: 0px 4px;
    max-width: 800px;
    white-space: pre-wrap;
    font-family: Consolas, Monaco, Andale Mono, monospace;
    line-height: 1.5;
    font-size: 13px;
    border: 1px solid #ddd;
    background-color: #f7f7f7;
    border-radius: 3px;
}
code {
    font-family: Consolas, Monaco, Andale Mono, monospace;
    line-height: 1.5;
    font-size: 13px;
    border: 1px solid #ddd;
    background-color: #f7f7f7;
    border-radius: 3px;
}
pre code {
    border: 0px;
}
aside {
    display: block;
    float: right;
    width: 390px;
}
blockquote {
    border-left:.5em solid #40AA53;
    padding: 0 2em;
    margin-left:0;
    max-width: 476px;
}
blockquote  cite {
    font-size:14px;
    line-height:20px;
    color:#bfbfbf;
}
blockquote cite:before {
    content: '\2014 \00A0';
}

blockquote p {  
    color: #666;
    max-width: 460px;
}
hr {
    height: 1px;
    border: none;
    border-top: 1px dashed #0066CC
}

button,
input,
select,
textarea {
  font-size: 100%;
  margin: 0;
  vertical-align: baseline;
  *vertical-align: middle;
}
button, input {
  line-height: normal;
  *overflow: visible;
}
button::-moz-focus-inner, input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
button,
input[type="button"],
input[type="reset"],
input[type="submit"] {
  cursor: pointer;
  -webkit-appearance: button;
}
input[type=checkbox], input[type=radio] {
  cursor: pointer;
}
/* override default chrome & firefox settings */
input:not([type="image"]), textarea {
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}

input[type="search"] {
  -webkit-appearance: textfield;
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
label,
input,
select,
textarea {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  font-weight: normal;
  line-height: normal;
  margin-bottom: 18px;
}
input[type=checkbox], input[type=radio] {
  cursor: pointer;
  margin-bottom: 0;
}
input[type=text],
input[type=password],
textarea,
select {
  display: inline-block;
  width: 210px;
  padding: 4px;
  font-size: 13px;
  font-weight: normal;
  line-height: 18px;
  height: 18px;
  color: #808080;
  border: 1px solid #ccc;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
}
select, input[type=file] {
  height: 27px;
  line-height: 27px;
}
textarea {
  height: auto;
}

/* grey out placeholders */
:-moz-placeholder {
  color: #bfbfbf;
}
::-webkit-input-placeholder {
  color: #bfbfbf;
}

input[type=text],
input[type=password],
select,
textarea {
  -webkit-transition: border linear 0.2s, box-shadow linear 0.2s;
  -moz-transition: border linear 0.2s, box-shadow linear 0.2s;
  transition: border linear 0.2s, box-shadow linear 0.2s;
  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
}
input[type=text]:focus, input[type=password]:focus, textarea:focus {
  outline: none;
  border-color: rgba(82, 168, 236, 0.8);
  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);
  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);
  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);
}

/* buttons */
button {
  display: inline-block;
  padding: 4px 14px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 18px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  border-radius: 4px;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);
  -moz-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);
  background-color: #0064cd;
  background-repeat: repeat-x;
  background-image: -khtml-gradient(linear, left top, left bottom, from(#049cdb), to(#0064cd));
  background-image: -moz-linear-gradient(top, #049cdb, #0064cd);
  background-image: -ms-linear-gradient(top, #049cdb, #0064cd);
  background-image: -webkit-gradient(linear, left top, left bottom, color-stop(0%, #049cdb), color-stop(100%, #0064cd));
  background-image: -webkit-linear-gradient(top, #049cdb, #0064cd);
  background-image: -o-linear-gradient(top, #049cdb, #0064cd);
  background-image: linear-gradient(top, #049cdb, #0064cd);
  color: #fff;
  text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.25);
  border: 1px solid #004b9a;
  border-bottom-color: #003f81;
  -webkit-transition: 0.1s linear all;
  -moz-transition: 0.1s linear all;
  transition: 0.1s linear all;
  border-color: #0064cd #0064cd #003f81;
  border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25);
}
button:hover {
  color: #fff;
  background-position: 0 -15px;
  text-decoration: none;
}
button:active {
  -webkit-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);
  -moz-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);
}
button::-moz-focus-inner {
  padding: 0;
  border: 0;
}
/* table  */
table {
    border-spacing: 0;
    border: 1px solid #ccc;
}
td, th{
    border: 1px solid #ccc;
    padding: 5px;
}
/* code syntax highlight.
Documentation: http://www.mdcharm.com/documentation/code_syntax_highlighting.html#custom_your_own
 */
pre .literal,
pre .comment,
pre .template_comment,
pre .diff .header,
pre .javadoc {
    color: #008000;
}

pre .keyword,
pre .css .rule .keyword,
pre .winutils,
pre .javascript .title,
pre .nginx .title,
pre .subst,
pre .request,
pre .status {
    color: #0000FF;
    font-weight: bold
}

pre .number,
pre .hexcolor,
pre .python .decorator,
pre .ruby .constant {
    color: #0000FF;
}

pre .string,
pre .tag .value,
pre .phpdoc,
pre .tex .formula {
    color: #D14
}

pre .title,
pre .id {
    color: #900;
    font-weight: bold
}

pre .javascript .title,
pre .lisp .title,
pre .clojure .title,
pre .subst {
    font-weight: normal
}

pre .class .title,
pre .haskell .type,
pre .vhdl .literal,
pre .tex .command {
    color: #458;
    font-weight: bold
}

pre .tag,
pre .tag .title,
pre .rules .property,
pre .django .tag .keyword {
    color: #000080;
    font-weight: normal
}

pre .attribute,
pre .variable,
pre .lisp .body {
    color: #008080
}

pre .regexp {
    color: #009926
}

pre .class {
    color: #458;
    font-weight: bold
}

pre .symbol,
pre .ruby .symbol .string,
pre .lisp .keyword,
pre .tex .special,
pre .prompt {
    color: #990073
}

pre .built_in,
pre .lisp .title,
pre .clojure .built_in {
    color: #0086b3
}

pre .preprocessor,
pre .pi,
pre .doctype,
pre .shebang,
pre .cdata {
    color: #999;
    font-weight: bold
}

pre .deletion {
    background: #fdd
}

pre .addition {
    background: #dfd
}

pre .diff .change {
    background: #0086b3
}

pre .chunk {
    color: #aaa
}

pre .markdown .header {
    color: #800;
    font-weight: bold;
}

pre .markdown .blockquote {
    color: #888;
}

pre .markdown .link_label {
    color: #88F;
}

pre .markdown .strong {
    font-weight: bold;
}

pre .markdown .emphasis {
    font-style: italic;
}

   </style>


		</head>
			<title>Ming Sun</title>
		<body>
		<div id="content">
		<h1>Ming Sun (孙明) </h1>
		<p><br>
		Kuaishou researcher, From 2021.3 to now	<br>
                Sensetime researcher, From 2018.10 to 2021.3 <br>
		BaiDu IDL intern and researcher, From 2016.11 to 2018.10 <br>
		Bytedance (TouTiao) AI Lab intern, From 2016.5 to 2016.10 <br>
		Email: m_sunming@163.com <br>
		</p>
		<p><img src='./Image/me.jpg', width='300'></p>
		<h2>Short Bio</h2>
        <p> From 2021, I am leading a outstanding group, about 20 researcher, which evaluate and promote the quality of user generated video on kuaishou.
	    I worked at SenseTime with <a href="https://yan-junjie.github.io/">Junjie Yan </a> from 2018.11 to 2021.3, focus on object detection and Automl.
            I worked at BaiDu IDL with <a href="http://f-zhou.com/">Feng Zhou</a> from 2016.11 to 2018.11 and learned a lot apart from tech. 
			Fortunately, I was supervised by prefessor <a href="http://cv.nankai.edu.cn/">JuFeng Yang</a> and cooperated with <a href="http://mmcheng.net/zh/cmm/">MingMing Cheng</a>.
			And received the Master degree from Nankai University in 2017.
		</p>
		<p>
			I was tech leader of the group, named 搜索与决策（inspired by exploration and exploitation), which focus on Detection（face/human/traffic/structure/keypoint/video） and Automl (augmentation/samping/loss/network auto search), about 20 researcher. 
		</p>
		<p>
			I am leading a group,  which focus on UGC video quality assessment (KVQ, Kuaishou Video Quality, such as noise/blocky/color/dirtylens ) and processing algorithms (KEP & KRP, Kuaishou Enhancement & Restoration Processing, such as video deblur/SR/denoise ), about 20 researcher. 
			For further boosting the performance of KVQ and KEP & KRP, we develop detection/classification algorithms.
		</p>

	        <h2>News:</h2>
		<ul>
	    <li> 通过大量的专家数据标注和模型改良，音视频质量领域大模型超过Golden eye.<a href=" https://mp.weixin.qq.com/s/1Lx1Iqly1-0ZubzDwei5SA"> KVQ算法 </a>.</li>
	    <li> One paper, named QPT,  was accepeted by CVPR 2023.<a href=" https://mp.weixin.qq.com/s/qvbwgfcJmO0x_uCV4fMHsQ"> 快手质量大模型 </a>.</li>
	    <li> 快手上线业界领先的端上画质超分算法，高端机AI方案+中端机ML方案. <a href=" https://mp.weixin.qq.com/s/c0VxPw3uVoWpelsYtn8wEg"> 软硬优化一体 </a>.</li>
	    <li> Three paper about transformer/nas were accepeted by ICCV 2021.</li>
	    <li> One paper about Autosampling was accepeted by ICML 2021.</li>
	    <li><a href=" https://github.com/Awesome-AutoAug-Algorithms/AWS-OHL-AutoAug">Best autoaug code are available!</a>.</li>
	    <li> Another paper about autoaug  were accepeted by NeurIPS 2020.</li>
	    <li> Two paper about nas  were accepeted by ECCV 2020.</li>
	    <li> Two paper about nas and detection were accepeted by CVPR 2020.</li>
	    <li> The team i lead get Dean Award （￥100,000）, which is the highest research reputation of sensetime .</li>
	    <li> Another paper about nas-detection was accepeted by ICLR 2020.</li>
	    <li> One paper about nas-detection was accepeted by NIPS 2019.</li> 
            <li> Another paper about detection was accepted by ICCV 2019.</li>
            <li> We won 1rd of Traffic Anomaly Detection and 3rd of City-Scale Multi-Camera Vehicle Tracking in CVPR 2019. </li>
            <li> One paper on visual emotion recognition was accepted to TOMM.</li>
            <li><a href="https://github.com/KaiyuYue/cgnl-network.pytorch">NIPS named CGNL code are available!</a>.</li>
			<li> Another paper was accepted in NIPS 2018. </li>
			<li> One paper accepted in ECCV 2018 with oral.</li>
			<li> Large scale (10,000+ classes) flower classification service can avaiable in BaiDu APP, which build on knowledge graph and  hundreds of millions data.</li>
			<li> We won 2rd flower and 3rd inaturalist in CVPR 2017 FGVC workshop.</li>
			<li> Got the Best New Artist and the Outstanding Project award in BaiDu IDL.</li>
			<li> One paper accepted in TMM 2018.</li>
			<li> Ming received National Scholarship for Graduate Students (￥20,000).</li>
			<li>Ming was named as Excellent Graduate, and received Outstanding Dissertations Award from Nankai University.</li>
		</ul>
		</div>
	       <h2>Publication</h2>
		<!-- --> 

		<blockquote>
			<p><a href="https://arxiv.org/pdf/2307.16813">
			Capturing Co-existing Distortions in User-Generated Content for No-reference Video Quality Assessment.</a><br />
			Kun Yuan, Zishang Kong, Chuanchuan Zheng, <strong> Ming Sun </strong>, Xing Wen. <br /> 
			ACM MM, 2023.  Sparse attention for VQA  <br /></p>
		</blockquote>
			
		<!-- --> 
			
		<blockquote>
			<p><a href="https://arxiv.org/abs/2307.08544">
			Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution.</a><br />
			Guandu Liu, Yukang Ding, Mading Li, <strong> Ming Sun </strong>, Xing Wen, Bin Wang. <br /> 
			ICCV, 2023.   <br /></p>
		</blockquote>
			
		<!-- --> 
			
		<blockquote>
			<p><a href="https://arxiv.org/abs/2303.00521">
			Quality-aware Pre-trained Models for Blind Image Quality Assessment.</a><br />
			Kai Zhao, Kun Yuan, <strong> Ming Sun </strong>, Mading Li, Xing Wen. <br /> 
			CVPR, 2023.  QPT with more data.  <br /></p>
		</blockquote>
			
		<!-- --> 
	
		<blockquote>
			<p><a href="https://arxiv.org/abs/2105.13695">
			AutoSampling: Search for Effective Data Sampling Schedules.</a><br />
			<strong> Ming Sun </strong>, Haoxuan Dou, Baopu Li, Junjie Yan, Wanli Ouyang, Lei Cui. <br /> 
			ICML, 2021.  Auto sampling firstly.  <br /></p>
		</blockquote>
		
		<!-- --> 
	
		<blockquote>
			<p><a href="https://arxiv.org/abs/2011.10904">
			Evolving Search Space for Neural Architecture Search.</a><br />
			Yuanzheng Ci, Chen Lin, <strong> Ming Sun </strong>, Boyu Chen, Hongwen Zhang, Wanli Ouyang , Junjie Yan. <br /> 
			ICCV, 2021. <br /> </a></p>
		</blockquote>
		
		<!-- --> 
	
		<blockquote>
			<p><a href="https://arxiv.org/abs/2107.02960">
			GLiT: Neural Architecture Search for Global and Local Image Transformer.</a><br />
			Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, <strong> Ming Sun </strong>, Junjie yan, Wanli Ouyang. <br /> 
			ICCV, 2021. <br /> </a></p>
		</blockquote>
		<!-- --> 
	
		<blockquote>
			<p><a href="https://arxiv.org/abs/2012.13587">
			Inception Convolution with Efficient Dilation Search.</a><br />
			Jie Liu, Chuming Li, Feng Liang, Chen Lin, <strong> Ming Sun </strong>, Junjie Yan, Wanli Ouyang, Dong Xu. <br /> 
			CVPR, 2021, <font size="3" color="red" > <strong> Oral </strong></font>.  Simple conv for scale variance  <br />
			<a href="https://github.com/yifan123/IC-Conv">IC-conv Code!</a></p>
		</blockquote>
			
		<!-- --> 
		<blockquote>
			<p><a href="https://arxiv.org/abs/2009.14737">
			Improving Auto-Augment via Augmentation-Wise Weight Sharing.</a><br />
			Keyu Tian, Chen Lin, <strong> Ming Sun </strong>, Luping Zhou, Junjie Yan, Wanli Ouyang <br /> 
			NeurIPS, 2020.  Best autoaug policy on Imagenet dataset  <br />
			<a href="https://github.com/Awesome-AutoAug-Algorithms/AWS-OHL-AutoAug">Autoaug Code!</a></p>
		</blockquote>
			
		<!-- --> 
			
        	<blockquote>
			<p><a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1942_ECCV_2020_paper.php">
			Efficient Transfer Learning via Joint Adaptation of Network Architecture and Weight.</a><br />
			<strong> Ming Sun </strong>, Haoxuan Dou, Junjie Yan <br /> 
			ECCV, 2020.  NAS for transfer learning first time!  <br /></p>
		</blockquote>
			
		<!-- --> 

        	<blockquote>
			<p><a href="https://arxiv.org/abs/2005.10511">
			Powering One-shot Topological NAS with Stabilized Share-parameter Proxy.</a><br />
			Ronghao Guo, Chen Lin, Chu ming Li, Keyu Tian, <strong> Ming Sun </strong>, Lu Sheng, Junjie Yan <br />
			ECCV, 2020.   NAS for Topo <br /></p>
		</blockquote>
		<!-- --> 

        	<blockquote>
			<p><a href="https://arxiv.org/abs/2005.08455">
			Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels.</a><br />
			Junran Peng, Xingyuan Bu, <strong> Ming Sun </strong>, Junjie Yan <br />
			CVPR, 2020, <font size="3" color="red" > <strong> Oral </strong></font>.  Simple trick for OD<br /></p>
		</blockquote>
	        <!-- --> 

        	<blockquote>
			<p><a href="https://arxiv.org/abs/1910.02543">
			Improving One-shot NAS by Suppressing the Posterior Fading.</a><br />
			Xiang Li, Chen Lin, Chuming Li, <strong> Ming Sun </strong>, Wei Wu, Junjie Yan, Wanli Ouyang <br />
			CVPR, 2020. <br /></p>
		</blockquote>
		<!-- -->
		<blockquote>
            		<p><a  href="https://openreview.net/forum?id=SkxLFaNKwB">
                	Computation Reallocation for Object Detection.</a><br />
            		Feng Liang, Ronghao Guo, Chen Lin, <strong>Ming Sun</strong>, Wei Wu, Junjie Yan, Wanli Ouyang <br />
            		ICLR, 2020. <br /></p>
        	</blockquote>
	       <!-- --> 

        	<blockquote>
			<p><a href="https://arxiv.org/abs/1909.02293">
			Efficient Neural Architecture Transformation Searchin Channel-Level for Object Detection.</a><br />
			Junran Peng,<strong> Ming Sun </strong>, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan <br />
			NIPS, 2019. <br /></p>
		</blockquote>
		<!-- -->
		<blockquote>
            		<p><a  href="https://arxiv.org/abs/1909.02225">
                	POD: Practical Object Detection with Scale-Sensitive Network.</a><br />
            		Junran Peng, <strong> Ming Sun </strong>, Zhaoxiang Zhang, Junjie Yan, Tieniu Tan <br />
            		ICCV, 2019. <br /></p>
        	</blockquote>
		<!-- -->
        	<blockquote>
            	<p> <a>
                	Learning discriminative sentiment representation from strongly- and weakly-supervised CNNs</a> <br/>
        		Dongyu She, <strong> Ming Sun </strong>, Jufeng Yang <br />
        		TOMM, 2019. <br /></p>
        	</blockquote>
		<!-- -->
		<blockquote>
		<p><a href="https://arxiv.org/abs/1806.05372">
			Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition</a> <br />
			<strong>Ming Sun</strong>, Yuchen Yuan, Feng Zhou, Errui Ding  <br/>
			ECCV, 2018, <font size="3" color="red" > <strong> Oral </strong></font>.<br /></p>
		</blockquote>
		<!-- -->
		<blockquote>
		<p><a href="https://arxiv.org/abs/1810.13125">
			Compact Generalized Non-local Network</a> <br />
			Kaiyu Yue, <strong>Ming Sun</strong>, Yuchen Yuan, Errui Ding,Fuxin Xu, Feng Zhou <br/>
			NIPS, 2018.<br />
        		<a href="https://github.com/KaiyuYue/cgnl-network.pytorch.">CGNL Code!</a></p>
		</blockquote>
                
		<!-- -->

		<blockquote>
			<p><a href="http://orca.cf.ac.uk/108453/1/visualsentiment.pdf">Visual Sentiment Prediction based on Automatic Discovery of Affective Regions </a> <br />
	Jufeng Yang, Dongyu She, <strong>Ming Sun</strong>, Ming-Ming Cheng, Paul L. Rosin, Liang Wang <br />
		 IEEE Transactions on Multimedia (TMM), 2018. <br />
		</blockquote>

                <!-- --> 

		<blockquote>
			<p><a href="./docs/2017-aaai-ldl.pdf">Learning Visual Sentiment Distributions via Augmented Conditional Probability Neural Network </a> <br />
		Jufeng Yang, <strong>Ming Sun</strong>, Xiaoxiao Sun <br />
		AAAI Conference on Artificial Intelligence (AAAI), 2017. <br />
		<a href="http://cv.nankai.edu.cn/projects/SentiLDL/">Dataset is available!</a></p>
		</blockquote>

                <!-- --> 

		<blockquote>
			<p><a href="http://www.ijcai.org/proceedings/2017/0456.pdf">Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network </a> <br />
		Jufeng Yang, Dongyu She,  <strong>Ming Sun</strong> <br />
                International Joint Conference on Artificial Intelligence (IJCAI), <font size="3" color="red" > <strong> Oral </strong></font>,2017. <br />
                </blockquote>

                <!-- --> 

		<blockquote>
			<p><a href="./docs/2016-eccv-sd198.pdf">A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images</a>  <br />
		Xiaoxiao Sun, Jufeng Yang, <strong>Ming Sun</strong>, Kai Wang <br />
		European Conference on Computer Vision (ECCV), 2016. <br />
		<a href="http://cv.nankai.edu.cn/projects/sd-198/">Project Homepage</a></p>
		</blockquote>
  
                <!-- --> 

		<blockquote>
		<p>Shape-Guided Segmentation for Fine-Grained Visual Categorization  <br />
		<strong>Ming Sun</strong>, Jufeng Yang, Bo Sun, Kai Wang  <br />
		IEEE International Conference on Multimedia and Expo (ICME), 2016, <font size="3" color="red" > <strong> Oral </strong></font>.<br /></p>
		</blockquote>
		
		<!-- -->

		<blockquote>
			<p><a href="./docs/icme2016discovering.pdf">Discovering Affective Regions in Deep Convolutional Neural Networks for Visual Sentiment Prediction </a><br />
		<strong>Ming Sun</strong>, Jufeng Yang, Kai Wang, Hui Shen  <br />
		IEEE International Conference on Multimedia and Expo (ICME), 2016.<br /></p>
		</blockquote>



              			
		</body>
	</html>






